# -*- coding: utf-8 -*-
"""Movie Genre Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EdvyhzAePAUTmxJcKmBqj9_Qf5Z832dk
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, classification_report,
                            confusion_matrix, f1_score)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
import seaborn as sns

# Download NLTK resources
nltk.download('wordnet')
nltk.download('stopwords')

# Data Loading and Preprocessing
def load_data(filepath):
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.split(':::')
            if len(parts) >= 4:
                movie_id = parts[0].strip()
                title = parts[1].strip()
                genre = parts[2].strip()
                plot = parts[3].strip()
                data.append({'id': movie_id, 'title': title, 'genre': genre, 'plot': plot})
    return pd.DataFrame(data)

# Text cleaning function
def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Text preprocessing with lemmatization
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Clean text
    text = clean_text(text)
    # Tokenize
    words = text.split()
    # Remove stopwords and lemmatize
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words)

# Main function
def main():
    # Load data (replace with your file path)
    filepath = '/content/train_data.txt'
    df = load_data(filepath)

    # Preprocess text
    df['processed_plot'] = df['plot'].apply(preprocess_text)

    # EDA: Genre distribution
    plt.figure(figsize=(10, 6))
    df['genre'].value_counts().plot(kind='bar')
    plt.title('Genre Distribution')
    plt.xlabel('Genre')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # Encode genres
    le = LabelEncoder()
    df['genre_encoded'] = le.fit_transform(df['genre'])

    # Split data
    X = df['processed_plot']
    y = df['genre_encoded']
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    # TF-IDF Vectorization
    tfidf = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        stop_words='english'
    )

    # Define classifiers to test
    classifiers = [
        ('Naive Bayes', MultinomialNB()),
        ('Logistic Regression', LogisticRegression(max_iter=1000)),
        ('SVM', SVC(kernel='linear')),
        ('Random Forest', RandomForestClassifier(n_estimators=100))
    ]

    # Evaluate each classifier
    results = []
    for name, clf in classifiers:
        # Create pipeline
        pipeline = Pipeline([
            ('tfidf', tfidf),
            ('clf', clf)
        ])

        # Cross-validation
        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(
            pipeline, X_train, y_train,
            cv=kfold, scoring='accuracy'
        )

        # Train and test
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)

        # Store results
        results.append({
            'model': name,
            'cv_mean_accuracy': np.mean(cv_scores),
            'cv_std_accuracy': np.std(cv_scores),
            'test_accuracy': accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'classification_report': classification_report(y_test, y_pred, target_names=le.classes_),
            'confusion_matrix': confusion_matrix(y_test, y_pred)
        })

        # Print results
        print(f"\n{name} Results:")
        print(f"CV Accuracy: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}")
        print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}")
        print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.3f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Compare models
    results_df = pd.DataFrame(results)
    print("\nModel Comparison:")
    print(results_df[['model', 'cv_mean_accuracy', 'test_accuracy', 'f1_score']])

    # Plot confusion matrix for best model
    best_model_idx = np.argmax(results_df['test_accuracy'])
    best_model_name = results_df.loc[best_model_idx, 'model']
    best_cm = results_df.loc[best_model_idx, 'confusion_matrix']

    plt.figure(figsize=(10, 8))
    sns.heatmap(best_cm, annot=True, fmt='d',
                xticklabels=le.classes_,
                yticklabels=le.classes_)
    plt.title(f'Confusion Matrix - {best_model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

    # Feature importance for interpretable models
    if best_model_name in ['Logistic Regression', 'Naive Bayes']:
        feature_names = tfidf.get_feature_names_out()

        if best_model_name == 'Logistic Regression':
            coefs = pipeline.named_steps['clf'].coef_
        else:  # Naive Bayes
            coefs = pipeline.named_steps['clf'].feature_log_prob_

        # Get top features for each class
        top_features = {}
        for i, genre in enumerate(le.classes_):
            if best_model_name == 'Logistic Regression':
                top_feature_indices = np.argsort(coefs[i])[-10:][::-1]
            else:
                top_feature_indices = np.argsort(coefs[i])[-10:][::-1]

            top_features[genre] = [feature_names[idx] for idx in top_feature_indices]

        print("\nTop Features by Genre:")
        for genre, features in top_features.items():
            print(f"\n{genre}:")
            print(", ".join(features))

if __name__ == "__main__":
    main()